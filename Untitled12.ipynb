{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMoAWzvZy/ggmssN2k5IQK3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/himanshu903411/EDA/blob/main/Untitled12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "e280sxeTrD86",
        "outputId": "471dcf4c-0b3e-44b5-ecdd-5a89e28252cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common seller type: Individual\n",
            "Number of bikes driven more than 50,000 km: 3\n",
            "Average km_driven for each ownership type:\n",
            " owner\n",
            "1st owner    18833.333333\n",
            "2nd owner    56666.666667\n",
            "3rd owner    85000.000000\n",
            "Name: km_driven, dtype: float64\n",
            "Proportion of bikes from 2015 or older: 60.00%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/mnt/data/EDA.pdf'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-b79b0922d1e5>\u001b[0m in \u001b[0;36m<cell line: 119>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;31m# Load the dataset from the uploaded file to check for missing values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/mnt/data/EDA.pdf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Assuming the file is a CSV or structured data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;31m# Checking for missing values in the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/EDA.pdf'"
          ]
        }
      ],
      "source": [
        "# 1)What is the range of selling prices in the dataset?\n",
        "\n",
        "#Identify the Column: Ensure that the column containing selling prices is clearly labeled, e.g., \"Selling Price.\"\n",
        "#Load the Dataset: If the file is provided, I can process it to extract the relevant column.\n",
        "#Calculate the Range:\n",
        "#Use the formula: Range = Maximum Price - Minimum Price.\n",
        "\n",
        "# 2) What is the median selling price for bikes in the dataset?\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset from the extracted table in the PDF document\n",
        "data = {\n",
        "    \"selling_price\": [\n",
        "        175000, 45000, 150000, 65000, 20000, 18000, 78500, 180000, 30000, 50000,\n",
        "        35000, 28000, 80000, 365000, 185000, 25000, 25000, 40000, 150000, 120000,\n",
        "        15000, 26000, 32000, 180000, 25000, 80000, 42000, 40000, 21000, 55000,\n",
        "        38000, 43000, 65000, 62000, 60000, 45000, 120000, 30000, 90000, 125000,\n",
        "        50000, 170000, 175000, 170000, 150000, 80000, 95000, 120000, 15000, 29900,\n",
        "        25000, 30000, 40000, 50000, 30000, 60000, 70000, 120000, 42000, 40000,\n",
        "        45000, 80000, 45000, 15000, 20000, 45000, 75000, 185000, 299000, 60000,\n",
        "        150000, 80000, 100000, 300000, 150000, 200000, 140000, 25000, 125000,\n",
        "        55000, 90000, 40000, 70000, 90000, 110000, 145000, 140000, 35000, 25000,\n",
        "        45000, 120000, 165000, 175000, 160000, 125000, 200000, 100000, 45000,\n",
        "        75000, 100000, 60000, 40000, 50000, 150000, 140000, 65000, 45000, 75000,\n",
        "        30000, 65000, 65000, 180000, 16000, 100000, 30000, 50000, 55000, 45000,\n",
        "        40000, 65000, 150000, 125000, 18000, 45000, 15000, 90000, 100000, 37000,\n",
        "        45000, 40000, 70000, 25000, 75000, 85000, 45000, 85000, 85000, 65000,\n",
        "        85000, 75000, 80000, 85000, 80000, 105000, 72000, 85000, 70000, 120000,\n",
        "        25000, 80000, 70000, 110000, 65000, 70000, 40000, 40000, 65000, 45000,\n",
        "        50000, 25000, 40000, 65000, 12000, 85000, 50000, 80000, 110000, 50000,\n",
        "        100000, 45000, 55000, 85000, 60000, 105000, 85000, 70000, 25000, 40000,\n",
        "        60000, 40000, 30000, 20000, 120000, 160000, 145000, 135000, 165000, 180000,\n",
        "        200000, 185000, 190000, 40000, 120000, 165000, 145000, 150000, 180000,\n",
        "        200000, 300000, 25000, 125000, 40000, 60000, 80000, 125000, 45000, 135000,\n",
        "        40000, 25000, 65000, 55000, 40000, 45000, 55000, 30000, 25000, 60000,\n",
        "        40000, 30000, 50000, 50000, 70000, 80000, 70000, 75000, 60000, 90000,\n",
        "        75000, 80000, 85000, 85000, 105000, 85000, 65000, 80000, 100000, 85000,\n",
        "        100000, 120000, 70000, 105000, 100000, 110000, 130000, 120000, 120000,\n",
        "        125000, 140000, 145000, 165000, 145000, 125000, 150000, 125000, 165000,\n",
        "        175000, 150000, 185000, 200000, 180000, 140000, 130000, 190000, 140000,\n",
        "        140000, 150000, 120000, 150000, 170000, 180000, 200000, 300000, 250000,\n",
        "        300000, 425000, 500000, 510000, 520000, 600000, 610000, 650000, 670000,\n",
        "        700000, 750000\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate the median selling price\n",
        "median_price = df['selling_price'].median()\n",
        "median_price\n",
        "\n",
        "\n",
        "# 3) What is the most common seller type?\n",
        "import pandas as pd\n",
        "\n",
        "# Example data for seller_type\n",
        "seller_type_data = [\n",
        "    \"Individual\", \"Individual\", \"Dealer\", \"Individual\", \"Dealer\", \"Individual\",\n",
        "    \"Individual\", \"Individual\", \"Dealer\", \"Individual\"\n",
        "]  # Add all your seller_type data here\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame({'seller_type': seller_type_data})\n",
        "\n",
        "# Find the most common seller type\n",
        "most_common_seller_type = df['seller_type'].mode()[0]\n",
        "print(\"Most common seller type:\", most_common_seller_type)\n",
        "\n",
        "\n",
        "# 4)How many bikes have driven more than 50,000 kilometers?\n",
        "import pandas as pd\n",
        "\n",
        "# Example data for km_driven\n",
        "km_driven_data = [\n",
        "    350, 5650, 12000, 60000, 21000, 50000, 42000, 85000, 32000, 60000\n",
        "]  # Add all your km_driven data here\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame({'km_driven': km_driven_data})\n",
        "\n",
        "# Count bikes with more than 50,000 kilometers driven\n",
        "bikes_over_50000_km = (df['km_driven'] > 50000).sum()\n",
        "print(\"Number of bikes driven more than 50,000 km:\", bikes_over_50000_km)\n",
        "\n",
        "\n",
        "# 5) What is the average km_driven value for each ownership type?\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Example data\n",
        "data = {\n",
        "    'km_driven': [350, 5650, 12000, 60000, 21000, 50000, 42000, 85000, 32000, 60000],  # Add your km_driven data here\n",
        "    'owner': ['1st owner', '1st owner', '1st owner', '2nd owner', '1st owner',\n",
        "              '2nd owner', '1st owner', '3rd owner', '1st owner', '2nd owner']  # Add your ownership data here\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate the average km_driven for each ownership type\n",
        "average_km_per_owner = df.groupby('owner')['km_driven'].mean()\n",
        "print(\"Average km_driven for each ownership type:\\n\", average_km_per_owner)\n",
        "\n",
        "# 6 ) What proportion of bikes are from the year 2015 or older?\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Example data\n",
        "data = {\n",
        "    'year': [2019, 2017, 2015, 2010, 2012, 2018, 2014, 2008, 2016, 2011]  # Add all your year data here\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate the proportion of bikes from the year 2015 or older\n",
        "proportion_older_bikes = (df['year'] <= 2015).mean()\n",
        "print(f\"Proportion of bikes from 2015 or older: {proportion_older_bikes:.2%}\")\n",
        "\n",
        "# 7 ) What is the trend of missing values across the dataset?\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset from the uploaded file to check for missing values\n",
        "file_path = '/mnt/data/EDA.pdf'\n",
        "df = pd.read_csv(file_path)  # Assuming the file is a CSV or structured data\n",
        "\n",
        "# Checking for missing values in the dataset\n",
        "missing_values_summary = df.isnull().sum()\n",
        "missing_values_summary\n",
        "\n",
        "\n",
        "# 9 ) What is the total number of bikes listed by each seller type?\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (replace 'file.csv' with your actual file name)\n",
        "df = pd.read_csv('file.csv')\n",
        "\n",
        "# Count the number of bikes listed by each seller type\n",
        "seller_counts = df['seller_type'].value_counts()\n",
        "\n",
        "# Print the results\n",
        "print(\"Total number of bikes listed by each seller type:\")\n",
        "print(seller_counts)\n",
        "\n",
        "# 10) What is the relationship between selling_price and km_driven for first-owner bikes?\n",
        "\n",
        "#To analyze the relationship between selling price and km driven for bikes listed as \"1st owner,\" we can extract and process the relevant data. Here's a summary approach:\n",
        "\n",
        "#Subset the Data: Focus on rows where the owner column is \"1st owner.\n",
        "#Scatter Plot: Create a scatter plot of selling_price (y-axis) vs. km_driven (x-axis) for \"1st owner\" bikes.\n",
        "#Correlation Analysis: Compute the correlation coefficient to assess the strength of the relationship.\n",
        "#Trend Line: Fit a regression line to evaluate trends (e.g., whether higher mileage leads to lower selling prices).\n",
        "\n",
        "# 11)  Identify and remove outliers in the km_driven column using the IQR method?\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (replace 'your_dataset.csv' with the actual file name)\n",
        "# Assuming the dataset is already loaded in a DataFrame called 'df'\n",
        "# Example:\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Calculate Q1, Q3, and IQR\n",
        "Q1 = df['km_driven'].quantile(0.25)\n",
        "Q3 = df['km_driven'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define the lower and upper bounds for outliers\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Identify outliers\n",
        "outliers = df[(df['km_driven'] < lower_bound) | (df['km_driven'] > upper_bound)]\n",
        "print(f\"Number of outliers: {len(outliers)}\")\n",
        "\n",
        "# Remove outliers\n",
        "df_cleaned = df[(df['km_driven'] >= lower_bound) & (df['km_driven'] <= upper_bound)]\n",
        "\n",
        "# Display summary\n",
        "print(f\"Original dataset size: {len(df)}\")\n",
        "print(f\"Cleaned dataset size: {len(df_cleaned)}\")\n",
        "\n",
        "# 12) Perform a bivariate analysis to visualize the relationship between year and selling_price?\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Simulating the dataset from the provided text (as an example since actual file isn't loaded as DataFrame)\n",
        "data = {\n",
        "    'year': [2019, 2017, 2018, 2015, 2011, 2010, 2018, 2008, 2010, 2016],\n",
        "    'selling_price': [175000, 45000, 150000, 65000, 20000, 18000, 78500, 180000, 30000, 50000]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Bivariate analysis: Year vs Selling Price\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(df['year'], df['selling_price'], alpha=0.7, color='blue', edgecolor='k')\n",
        "plt.title('Bivariate Analysis: Year vs Selling Price', fontsize=14)\n",
        "plt.xlabel('Year of Manufacturing', fontsize=12)\n",
        "plt.ylabel('Selling Price (INR)', fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n",
        "\n",
        "# 13) What is the average depreciation in selling price based on the bike's age (current year - manufacturing year)?\n",
        "from datetime import datetime\n",
        "\n",
        "# Adding current year and calculating bike's age\n",
        "current_year = datetime.now().year\n",
        "df['age'] = current_year - df['year']\n",
        "\n",
        "# Calculating depreciation (assuming depreciation = selling price reduction per year of age)\n",
        "df['depreciation_per_year'] = df['selling_price'] / df['age']\n",
        "\n",
        "# Calculating the average depreciation per year\n",
        "average_depreciation = df['depreciation_per_year'].mean()\n",
        "average_depreciation\n",
        "\n",
        "# 14) Which bike names are priced significantly above the average price for their manufacturing year?\n",
        "\n",
        "# Adding bike names to the dataset for analysis (example data, as the original names were not included)\n",
        "df['bike_name'] = [\n",
        "    \"Royal Enfield Classic 350\", \"Honda Dio\", \"Royal Enfield Classic Gunmetal Grey\",\n",
        "    \"Yamaha Fazer FI V 2.0\", \"Yamaha SZ\", \"Honda CB Twister\", \"Honda CB Hornet 160R\",\n",
        "    \"Royal Enfield Bullet 350\", \"Hero Honda CBZ extreme\", \"Bajaj Discover 125\"\n",
        "]\n",
        "\n",
        "# Calculating average price per manufacturing year\n",
        "average_price_by_year = df.groupby('year')['selling_price'].mean()\n",
        "\n",
        "# Identifying bikes priced significantly above the average for their year\n",
        "threshold = 1.5  # Setting a threshold for \"significantly above\" as 50% above the average\n",
        "df['is_significant'] = df.apply(\n",
        "    lambda row: row['selling_price'] > threshold * average_price_by_year[row['year']], axis=1\n",
        ")\n",
        "\n",
        "# Extracting bikes with significant pricing\n",
        "significant_bikes = df[df['is_significant']][['bike_name', 'year', 'selling_price']]\n",
        "significant_bikes\n",
        "\n",
        "# 15) Develop a correlation matrix for numeric columns and visualize it using a heatmap?\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Selecting numeric columns for correlation analysis\n",
        "numeric_columns = ['year', 'selling_price', 'age']\n",
        "correlation_matrix = df[numeric_columns].corr()\n",
        "\n",
        "# Visualizing the correlation matrix using a heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', cbar=True)\n",
        "plt.title('Correlation Matrix for Numeric Columns', fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "# EDA - 2\n",
        "# About - Dataset:\n",
        "\n",
        "# 1) What is the average selling price of cars for each dealer, and how does it compare across different dealers?\n",
        "\n",
        "# 1)Load the Dataset: Use a data processing tool like Python's pandas library or Excel.\n",
        "# 2)Group by Dealer: Group the data by Dealer_Name.\n",
        "# 3)Calculate the Average: Compute the mean of the Price ($) for each dealer.\n",
        "# 4)Compare Across Dealers: Sort the results to identify trends or differences between dealers.\n",
        "\n",
        "# 2) Which car brand (Company) has the highest variation in prices, and what does this tell us about the pricing trends?\n",
        "\n",
        "#Steps to Analyze:\n",
        "#Load the Dataset:\n",
        "\n",
        "#Use a tool like Python, Excel, or R to load the dataset.\n",
        "#Group Data by Company:\n",
        "\n",
        "#Group the dataset by the Company column.\n",
        "#Calculate Price Variation:\n",
        "\n",
        "#For each company, compute the standard deviation of the Price ($) column. This represents how spread out the prices are for that brand.\n",
        "#Identify the Brand with the Highest Variation:\n",
        "\n",
        "#Sort the companies by their price standard deviation in descending order. The company with the highest standard deviation has the most price variation.\n",
        "#Interpret Results:\n",
        "\n",
        "#High variation might indicate that the company offers cars at both lower and premium price points, catering to a wide market.\n",
        "#Low variation might indicate a focus on a specific market segment.\n",
        "#What This Reveals About Pricing Trends:\n",
        "#High Variation: Suggests a diverse product lineup, potentially targeting both budget-conscious and luxury-oriented customers.\n",
        "#Low Variation: Suggests a more focused pricing strategy, likely catering to a specific demographic.\n",
        "\n",
        "# 3)What is the distribution of car prices for each transmission type, and how do the interquartile ranges compare?\n",
        "\n",
        "#Steps for Analysis:\n",
        "#Load the Dataset:\n",
        "\n",
        "#Use tools like Python (pandas and matplotlib/seaborn), Excel, or R.\n",
        "#Filter by Transmission Type:\n",
        "\n",
        "#Separate the data into groups by Transmission (e.g., \"Manual\" and \"Automatic\").\n",
        "#Summarize the Price Distribution:\n",
        "\n",
        "#Calculate key summary statistics for the Price ($) column for each transmission type:\n",
        "#Minimum\n",
        "#1st Quartile (Q1)\n",
        "#Median\n",
        "#3rd Quartile (Q3)\n",
        "#Maximum\n",
        "#IQR:\n",
        "#IQR=ùëÑ3‚àíùëÑ1\n",
        "#IQR=Q3‚àíQ1\n",
        "#Visualize the Distribution:\n",
        "\n",
        "#Create boxplots to visualize the distribution of prices for \"Manual\" and \"Automatic\" transmissions.\n",
        "#Compare the IQRs:\n",
        "\n",
        "#Larger IQR indicates more variability in prices within that transmission type.\n",
        "#Smaller IQR indicates more consistent pricing.\n",
        "#What This Reveals:\n",
        "#Wide IQR:\n",
        "#A wide range of prices suggests diverse options, catering to different budgets or car types.\n",
        "#Narrow IQR:\n",
        "#Consistent pricing suggests a more uniform market segment for that transmission type.\n",
        "\n",
        "# 4) What is the distribution of car prices across different regions?\n",
        "\n",
        "#Steps for Analysis:\n",
        "#Load the Dataset:\n",
        "\n",
        "#Use a tool like Python, Excel, or R to import the dataset.\n",
        "#Group by Region:\n",
        "\n",
        "#Group the data by the Dealer_Region column.\n",
        "#Summarize Price Distribution:\n",
        "\n",
        "#For each region, calculate key statistics for the Price ($) column:\n",
        "#Minimum, Maximum, Mean, Median, and Standard Deviation.\n",
        "#Quartiles (Q1, Q3) and Interquartile Range (IQR).\n",
        "#Visualize the Distribution:\n",
        "\n",
        "#Use visualization tools such as:\n",
        "#Boxplots: To compare price distributions across regions.\n",
        "#Histograms/Violin Plots: To display the density of prices in each region.\n",
        "#Analyze Results:\n",
        "\n",
        "#Compare regions in terms of:\n",
        "#Average prices: Which regions have higher/lower mean prices?\n",
        "#Variability: Which regions show broader price ranges or higher variability?\n",
        "#Quartiles and IQR: Which regions are more consistent in pricing?\n",
        "\n",
        "# 5 ) What is the distribution of cars based on body styles?\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('your_dataset.csv')  # Replace with your dataset file path\n",
        "\n",
        "# Count the number of cars for each body style\n",
        "body_style_counts = data['Body Style'].value_counts()\n",
        "\n",
        "# Plot the distribution\n",
        "body_style_counts.plot(kind='bar', color='skyblue', figsize=(10, 6))\n",
        "plt.title('Distribution of Cars by Body Style')\n",
        "plt.xlabel('Body Style')\n",
        "plt.ylabel('Number of Cars')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# 6 ) How does the average selling price of cars vary by customer gender and annual income?\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('your_dataset.csv')  # Replace with your dataset path\n",
        "\n",
        "# Create income brackets\n",
        "income_bins = [0, 50000, 100000, 500000, 1000000, data['Annual Income'].max()]\n",
        "income_labels = ['Low', 'Lower-Middle', 'Middle', 'Upper-Middle', 'High']\n",
        "data['Income Bracket'] = pd.cut(data['Annual Income'], bins=income_bins, labels=income_labels)\n",
        "\n",
        "# Group by Gender and Income Bracket\n",
        "grouped = data.groupby(['Gender', 'Income Bracket'])['Price ($)'].mean().unstack()\n",
        "\n",
        "# Plot the data\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(grouped, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
        "plt.title('Average Selling Price by Gender and Income Bracket')\n",
        "plt.ylabel('Gender')\n",
        "plt.xlabel('Income Bracket')\n",
        "plt.show()\n",
        "\n",
        "# 7) What is the distribution of car prices by region, and how does the number of cars sold vary by region?\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('your_dataset.csv')  # Replace with your file path\n",
        "\n",
        "# Distribution of car prices by region\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x='Dealer_Region', y='Price ($)', data=data, palette=\"Set2\")\n",
        "plt.title('Distribution of Car Prices by Region')\n",
        "plt.xlabel('Region')\n",
        "plt.ylabel('Price ($)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Number of cars sold by region\n",
        "car_counts = data['Dealer_Region'].value_counts()\n",
        "car_counts.plot(kind='bar', color='skyblue', figsize=(10, 6))\n",
        "plt.title('Number of Cars Sold by Region')\n",
        "plt.xlabel('Region')\n",
        "plt.ylabel('Number of Cars Sold')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 8 ) How does the average car price differ between cars with different engine sizes?\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('your_dataset.csv')  # Replace with your dataset path\n",
        "\n",
        "# Calculate average price by engine type\n",
        "average_prices = data.groupby('Engine')['Price ($)'].mean().sort_values(ascending=False)\n",
        "\n",
        "# Plot the average prices\n",
        "plt.figure(figsize=(10, 6))\n",
        "average_prices.plot(kind='bar', color='skyblue')\n",
        "plt.title('Average Car Price by Engine Size')\n",
        "plt.xlabel('Engine Size')\n",
        "plt.ylabel('Average Price ($)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 9) How do car prices vary based on the customer‚Äôs annual income bracket?\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('your_dataset.csv')  # Replace with your dataset path\n",
        "\n",
        "# Create income brackets\n",
        "income_bins = [0, 50000, 100000, 500000, 1000000, data['Annual Income'].max()]\n",
        "income_labels = ['Low', 'Lower-Middle', 'Middle', 'Upper-Middle', 'High']\n",
        "data['Income Bracket'] = pd.cut(data['Annual Income'], bins=income_bins, labels=income_labels)\n",
        "\n",
        "# Calculate average car prices by income bracket\n",
        "average_prices = data.groupby('Income Bracket')['Price ($)'].mean()\n",
        "\n",
        "# Plot average prices by income bracket\n",
        "plt.figure(figsize=(10, 6))\n",
        "average_prices.plot(kind='bar', color='skyblue')\n",
        "plt.title('Average Car Price by Customer Income Bracket')\n",
        "plt.xlabel('Income Bracket')\n",
        "plt.ylabel('Average Price ($)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Optional: Boxplot for price variability\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='Income Bracket', y='Price ($)', data=data, palette=\"Set2\")\n",
        "plt.title('Car Price Variability by Customer Income Bracket')\n",
        "plt.xlabel('Income Bracket')\n",
        "plt.ylabel('Car Price ($)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# 10) What are the top 5 car models with the highest number of sales, and how does their price distribution look?\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('your_dataset.csv')  # Replace with your dataset path\n",
        "\n",
        "# Count sales by model\n",
        "model_sales = data['Model'].value_counts()\n",
        "\n",
        "# Get the top 5 models\n",
        "top_5_models = model_sales.head(5).index\n",
        "\n",
        "# Filter the data for top 5 models\n",
        "top_5_data = data[data['Model'].isin(top_5_models)]\n",
        "\n",
        "# Plot the price distribution for the top 5 models\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x='Model', y='Price ($)', data=top_5_data, palette=\"Set3\")\n",
        "plt.title('Price Distribution for Top 5 Car Models')\n",
        "plt.xlabel('Car Model')\n",
        "plt.ylabel('Price ($)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Optional: Display sales count for top 5 models\n",
        "print(\"Top 5 Models by Sales:\")\n",
        "print(model_sales.head(5))\n",
        "\n",
        "# 11) How does car price vary with engine size across different car colors, and which colors have the highest price variation?\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming 'df' is your dataset\n",
        "# Grouping by color and engine size\n",
        "color_engine_group = df.groupby(['Color', 'Engine'])['Price ($)'].mean().unstack()\n",
        "\n",
        "# Plotting price distribution by color\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x='Color', y='Price ($)', data=df, palette='Set2')\n",
        "plt.title('Price Variation Across Car Colors', fontsize=16)\n",
        "plt.xlabel('Car Color', fontsize=12)\n",
        "plt.ylabel('Price ($)', fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Calculating standard deviation of price by color\n",
        "price_variation = df.groupby('Color')['Price ($)'].std().sort_values(ascending=False)\n",
        "print(\"Colors with the highest price variation:\")\n",
        "print(price_variation)\n",
        "\n",
        "# 12 ) Is there any seasonal trend in car sales based on the date of sale?\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure 'Date' column is in datetime format\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Extracting month and season\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Season'] = df['Date'].dt.month % 12 // 3 + 1  # Mapping months to seasons\n",
        "\n",
        "# Aggregating sales data by month\n",
        "monthly_sales = df.groupby('Month').size()\n",
        "\n",
        "# Aggregating sales data by season\n",
        "seasonal_sales = df.groupby('Season').size()\n",
        "\n",
        "# Visualizing monthly sales trend\n",
        "plt.figure(figsize=(12, 6))\n",
        "monthly_sales.plot(kind='bar', color='skyblue', alpha=0.8)\n",
        "plt.title('Monthly Car Sales Trend', fontsize=16)\n",
        "plt.xlabel('Month', fontsize=12)\n",
        "plt.ylabel('Number of Cars Sold', fontsize=12)\n",
        "plt.xticks(ticks=range(12), labels=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
        "                                    'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], rotation=45)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# Visualizing seasonal sales trend\n",
        "plt.figure(figsize=(8, 5))\n",
        "seasonal_sales.plot(kind='bar', color='orange', alpha=0.8)\n",
        "plt.title('Seasonal Car Sales Trend', fontsize=16)\n",
        "plt.xlabel('Season (1: Winter, 2: Spring, 3: Summer, 4: Autumn)', fontsize=12)\n",
        "plt.ylabel('Number of Cars Sold', fontsize=12)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 13)  How does the car price distribution change when considering different combinations of body style and transmission type?\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'df' is your dataset\n",
        "# Grouping by body style and transmission type\n",
        "df['Combination'] = df['Body Style'] + \" - \" + df['Transmission']\n",
        "\n",
        "# Box plot to visualize price distribution\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.boxplot(x='Combination', y='Price ($)', data=df, palette='Set3')\n",
        "plt.title('Car Price Distribution by Body Style and Transmission Type', fontsize=16)\n",
        "plt.xlabel('Body Style - Transmission', fontsize=12)\n",
        "plt.ylabel('Price ($)', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Optional: Calculate summary statistics\n",
        "summary_stats = df.groupby(['Body Style', 'Transmission'])['Price ($)'].describe()\n",
        "print(summary_stats)\n",
        "\n",
        "# 14) What is the correlation between car price, engine size, and annual income of customers, and how do these features interact?\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'df' is your dataset\n",
        "# Selecting relevant numeric columns\n",
        "numeric_features = ['Price ($)', 'Engine Size', 'Annual Income']\n",
        "correlation_matrix = df[numeric_features].corr()\n",
        "\n",
        "# Heatmap for correlation matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Correlation Matrix: Price, Engine Size, and Annual Income', fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "# Pair plot to visualize interactions\n",
        "sns.pairplot(df[numeric_features], diag_kind='kde', corner=True)\n",
        "plt.suptitle('Pair Plot: Price, Engine Size, and Annual Income', y=1.02, fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "# 3D scatter plot (optional)\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(df['Engine Size'], df['Annual Income'], df['Price ($)'], c='b', alpha=0.6, edgecolor='k')\n",
        "ax.set_xlabel('Engine Size')\n",
        "ax.set_ylabel('Annual Income')\n",
        "ax.set_zlabel('Price ($)')\n",
        "ax.set_title('3D Scatter: Engine Size vs Annual Income vs Price')\n",
        "plt.show()\n",
        "\n",
        "# 15) How does the average car price vary across different car models and engine types?\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'df' is your dataset\n",
        "# Grouping by Model and Engine Type to calculate average price\n",
        "avg_price = df.groupby(['Model', 'Engine'])['Price ($)'].mean().reset_index()\n",
        "\n",
        "# Heatmap visualization for better insights\n",
        "pivot_table = avg_price.pivot('Model', 'Engine', 'Price ($)')\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(pivot_table, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
        "plt.title('Average Car Price by Model and Engine Type', fontsize=14)\n",
        "plt.xlabel('Engine Type', fontsize=12)\n",
        "plt.ylabel('Car Model', fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Optional: Bar chart for a specific engine type\n",
        "specific_engine = \"V6\"  # Replace with the desired engine type\n",
        "engine_filtered = avg_price[avg_price['Engine'] == specific_engine]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=engine_filtered, x='Model', y='Price ($)', palette='Set2')\n",
        "plt.title(f'Average Price by Car Model for Engine Type: {specific_engine}', fontsize=14)\n",
        "plt.xlabel('Car Model', fontsize=12)\n",
        "plt.ylabel('Average Price ($)', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# EDA 3\n",
        "\n",
        "# 1)  What is the average rating for each product category?\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define the structure based on the dataset description in the document\n",
        "data = {\n",
        "    'category': [\n",
        "        \"Computers&Accessories|Accessories&Peripherals|Cables&Accessories|Cables|USBCables\",\n",
        "        \"Computers&Accessories|Accessories&Peripherals|Cables&Accessories|Cables|HDMICables\",\n",
        "        \"Electronics|HomeTheater,TV&Video|Accessories|Cables|HDMICables\",\n",
        "        \"Computers&Accessories|Accessories&Peripherals|Cables&Accessories|Cables|USBCables\",\n",
        "        \"Electronics|HomeTheater,TV&Video|Televisions|SmartTelevisions\"\n",
        "    ],\n",
        "    'rating': [4.2, 4.4, 4.5, 4.0, 4.3]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Group by category and calculate the average rating\n",
        "average_ratings = df.groupby('category')['rating'].mean()\n",
        "\n",
        "# Display the results\n",
        "average_ratings\n",
        "\n",
        "# 2)  What are the top rating_count products by category?\n",
        "\n",
        "# Extending the dataset with rating_count for demonstration purposes\n",
        "data_extended = {\n",
        "    'category': [\n",
        "        \"Computers&Accessories|Accessories&Peripherals|Cables&Accessories|Cables|USBCables\",\n",
        "        \"Computers&Accessories|Accessories&Peripherals|Cables&Accessories|Cables|HDMICables\",\n",
        "        \"Electronics|HomeTheater,TV&Video|Accessories|Cables|HDMICables\",\n",
        "        \"Computers&Accessories|Accessories&Peripherals|Cables&Accessories|Cables|USBCables\",\n",
        "        \"Electronics|HomeTheater,TV&Video|Televisions|SmartTelevisions\"\n",
        "    ],\n",
        "    'rating': [4.2, 4.4, 4.5, 4.0, 4.3],\n",
        "    'rating_count': [24269, 426973, 107687, 43994, 32999],\n",
        "    'product_name': [\n",
        "        \"Wayona Nylon Braided USB Cable\",\n",
        "        \"AmazonBasics Flexible HDMI Cable\",\n",
        "        \"Tizum High-Speed HDMI Cable\",\n",
        "        \"Ambrane Fast Charging Cable\",\n",
        "        \"Samsung Smart LED TV\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_extended = pd.DataFrame(data_extended)\n",
        "\n",
        "# Sort within each category by rating_count and select the top product\n",
        "top_products = df_extended.loc[df_extended.groupby('category')['rating_count'].idxmax()]\n",
        "\n",
        "# Display the results\n",
        "top_products[['category', 'product_name', 'rating_count']]\n",
        "\n",
        "\n",
        "# 3)  What is the distribution of discounted prices vs. actual prices?\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extending the dataset with prices for demonstration purposes\n",
        "data_prices = {\n",
        "    'category': [\n",
        "        \"Computers&Accessories|Accessories&Peripherals|Cables&Accessories|Cables|USBCables\",\n",
        "        \"Computers&Accessories|Accessories&Peripherals|Cables&Accessories|Cables|HDMICables\",\n",
        "        \"Electronics|HomeTheater,TV&Video|Accessories|Cables|HDMICables\",\n",
        "        \"Computers&Accessories|Accessories&Peripherals|Cables&Accessories|Cables|USBCables\",\n",
        "        \"Electronics|HomeTheater,TV&Video|Televisions|SmartTelevisions\"\n",
        "    ],\n",
        "    'discounted_price': [399, 219, 209, 199, 13490],\n",
        "    'actual_price': [1099, 700, 695, 499, 22990]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_prices = pd.DataFrame(data_prices)\n",
        "\n",
        "# Plotting the distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(df_prices['actual_price'], df_prices['discounted_price'], color='blue', alpha=0.7, label='Discounted vs. Actual Prices')\n",
        "plt.plot([0, max(df_prices['actual_price'])], [0, max(df_prices['actual_price'])], color='red', linestyle='--', label='Equality Line')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.title(\"Discounted Prices vs. Actual Prices Distribution\", fontsize=14)\n",
        "plt.xlabel(\"Actual Price (‚Çπ)\", fontsize=12)\n",
        "plt.ylabel(\"Discounted Price (‚Çπ)\", fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 4) How does the average discount percentage vary across categories ?\n",
        "\n",
        "# Calculate average discount percentage by category from the extended dataset\n",
        "average_discounts = df_discounts.groupby('category')['discount_percentage'].mean()\n",
        "\n",
        "# Display the results\n",
        "average_discounts\n",
        "\n",
        "# 5) What are the most popular product names?\n",
        "\n",
        "#Most Popular Products (by Rating Count)\n",
        "#AmazonBasics Flexible HDMI Cable\n",
        "\n",
        "#Rating Count: 426,973\n",
        "#Tizum High-Speed HDMI Cable\n",
        "\n",
        "#Rating Count: 107,687\n",
        "#Ambrane Fast Charging Cable\n",
        "\n",
        "#Rating Count: 43,994\n",
        "#Samsung Smart LED TV\n",
        "\n",
        "#Rating Count: 32,999\n",
        "\n",
        "#6) What are the most popular product keywords ?\n",
        "\n",
        "#Product Names:\n",
        "#Wayona Nylon Braided USB Cable\n",
        "#AmazonBasics Flexible HDMI Cable\n",
        "#Tizum High-Speed HDMI Cable\n",
        "#Ambrane Fast Charging Cable\n",
        "#Samsung Smart LED TV\n",
        "\n",
        "#Extracted Keywords (Top 5 by Frequency):\n",
        "#Cable: Appears 4 times (high relevance due to multiple cables in dataset).\n",
        "#HDMI: Appears 2 times.\n",
        "#USB: Appears 2 times.\n",
        "#Braided: Appears 1 time (popular among durable products).\n",
        "#Charging: Appears 1 time (significant for chargers and cables).\n",
        "\n",
        "#7) ' What are the most popular product reviews?\n",
        "\n",
        "#High Rating Counts: Products with more rating_count likely have more popular reviews.\n",
        "#Engagement Indicators: If the dataset includes details like review_content or review_title, we could analyze the reviews' content length or specific terms (e.g., \"excellent,\" \"poor\") for popularity or sentiment.\n",
        "#Based on Available Data:\n",
        "#The most popular reviews would correspond to the products with the highest rating_count.\n",
        "#For example:\n",
        "#AmazonBasics Flexible HDMI Cable (Rating Count: 426,973)\n",
        "#Tizum High-Speed HDMI Cable (Rating Count: 107,687)\n",
        "\n",
        "# 8) What is the correlation between discounted_price and rating?\n",
        "\n",
        "# Extend the dataset with both discounted_price and rating for demonstration purposes\n",
        "data_correlation = {\n",
        "    'discounted_price': [399, 219, 209, 199, 13490],\n",
        "    'rating': [4.2, 4.4, 4.5, 4.0, 4.3]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_correlation = pd.DataFrame(data_correlation)\n",
        "\n",
        "# Calculate the correlation between discounted_price and rating\n",
        "correlation = df_correlation['discounted_price'].corr(df_correlation['rating'])\n",
        "\n",
        "# Display the correlation\n",
        "correlation\n",
        "\n",
        "\n",
        "#9)  What are the Top 5 categories based on the highest rating?\n",
        "\n",
        "#To find the Top 5 categories based on the highest ratings, we would:\n",
        "\n",
        "#Group products by their category.\n",
        "#Compute the average rating for each category.\n",
        "#Sort the categories in descending order of the average rating.\n",
        "#Pick the top 5.\n",
        "\n",
        "# 10)  Identify any potential areas for improvement or optimization based on the data analysis ?\n",
        "\n",
        "#1. Product Pricing and Discounts\n",
        "#Observation: The dataset includes discounted_price, actual_price, and discount_percentage.\n",
        "#Improvement Area:\n",
        "#Analyze whether higher discounts correlate with higher ratings or sales. Optimize discounts to strike a balance between profitability and customer satisfaction.\n",
        "#Identify products with consistently low ratings but high discounts to reconsider pricing strategies or improve product quality.\n",
        "#2. Customer Reviews and Feedback\n",
        "#Observation: The dataset includes review_title, review_content, and rating.\n",
        "#Improvement Area:\n",
        "#Use Natural Language Processing (NLP) to analyze the sentiment of reviews. Highlight common customer complaints or recurring issues.\n",
        "#Focus on products with high ratings but few reviews to encourage more feedback, possibly by offering incentives for reviews.\n",
        "#3. Category Performance\n",
        "#Observation: The dataset categorizes products into distinct groups.\n",
        "#Improvement Area:\n",
        "#Identify underperforming categories based on average ratings and sales volume. Allocate resources (marketing, R&D) to boost these categories.\n",
        "#Determine best-performing categories and replicate successful strategies across weaker ones.\n",
        "#4. Rating Discrepancies\n",
        "#Observation: Ratings and rating_count are provided.\n",
        "#Improvement Area:\n",
        "#Investigate categories or products with high ratings but low rating_count to ensure the ratings are representative.\n",
        "#Improve visibility and engagement for products with low ratings and identify underlying causes (e.g., quality, usability).\n",
        "#5. Customer Segmentation\n",
        "#Observation: The dataset includes user_id and user_name.\n",
        "#Improvement Area:\n",
        "#Perform user segmentation to identify loyal customers or high-value users. Create targeted campaigns to increase their retention or purchases.\n",
        "#Identify users who frequently leave low ratings and resolve their concerns proactively.\n",
        "#6. Product Descriptions\n",
        "#Observation: The dataset has about_product field for descriptions.\n",
        "#Improvement Area:\n",
        "#Review the quality of product descriptions, ensuring clarity, accuracy, and alignment with customer needs. Poor descriptions might lead to lower conversion rates.\n",
        "#7. Visual and Navigational Optimization\n",
        "#Observation: The dataset includes img_link and product_link.\n",
        "#Improvement Area:\n",
        "#Evaluate the quality of images and ensure product pages are engaging and provide necessary information. Poor visuals or navigation might deter purchases.\n",
        "#8. Outlier Analysis\n",
        "#Observation: The dataset includes a range of products with varying ratings and prices.\n",
        "#Improvement Area:\n",
        "#Identify outliers (e.g., products with very low ratings or significantly fewer sales) and determine whether to improve, replace, or discontinue them.\n",
        "#9. Inventory Management\n",
        "#Observation: Products with poor ratings might be clogging inventory.\n",
        "#Improvement Area:\n",
        "#Optimize inventory based on sales trends and customer feedback, ensuring high-demand, well-rated products are adequately stocked.\n",
        "#10. Enhancing Customer Service\n",
        "#Observation: Some products have warranty or customer service mentions in the about_product.\n",
        "#Improvement Area:\n",
        "#Analyze whether warranty and support services impact ratings and adjust offerings accordingly.\n",
        "#Proactively communicate warranty and service benefits to customers.\n",
        "\n",
        "# EDA 4\n",
        "\n",
        "#1) Read the dataframe, check null value if present then do the needful, check duplicate row , if present then do the needful?\n",
        "\n",
        "#Steps to Process the Data:\n",
        "#Read the Data: Load the dataset into a pandas DataFrame for analysis.\n",
        "#Check for Null Values:\n",
        "#Identify columns with null values.\n",
        "#Decide on a strategy: fill missing values (e.g., with mean/median/mode) or remove rows/columns based on the context and proportion of missing data.\n",
        "#Check for Duplicate Rows:\n",
        "#Identify duplicate rows using pandas.DataFrame.duplicated().\n",
        "#Remove duplicates to avoid redundancy in the dataset.\n",
        "\n",
        "#2) What is the distribution of popularity among the tracks in the dataset? Visualize it using a histogram ?\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "# Assuming the dataset is named 'spotify_tracks.csv'\n",
        "df = pd.read_csv('spotify_tracks.csv')\n",
        "\n",
        "# Check for null values in 'Popularity' column\n",
        "if df['Popularity'].isnull().sum() > 0:\n",
        "    print(\"Null values found in 'Popularity'. Filling with median value.\")\n",
        "    df['Popularity'] = df['Popularity'].fillna(df['Popularity'].median())\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(df['Popularity'], bins=20, color='skyblue', edgecolor='black')\n",
        "plt.title('Distribution of Track Popularity')\n",
        "plt.xlabel('Popularity')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "#3) Is there any relationship between the popularity and the duration of tracks? Explore this using a scatter plot?\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "# Assuming the dataset is named 'spotify_tracks.csv'\n",
        "df = pd.read_csv('spotify_tracks.csv')\n",
        "\n",
        "# Check for null values in relevant columns\n",
        "if df[['Popularity', 'Duration (ms)']].isnull().any().any():\n",
        "    print(\"Null values found. Dropping rows with null values.\")\n",
        "    df = df.dropna(subset=['Popularity', 'Duration (ms)'])\n",
        "\n",
        "# Scatter plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=df['Duration (ms)'], y=df['Popularity'], color='blue', alpha=0.6)\n",
        "\n",
        "# Enhancing the plot\n",
        "plt.title('Scatter Plot: Popularity vs Duration of Tracks')\n",
        "plt.xlabel('Duration (ms)')\n",
        "plt.ylabel('Popularity')\n",
        "plt.grid(axis='both', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Optional: Add a trend line (linear regression)\n",
        "sns.regplot(x=df['Duration (ms)'], y=df['Popularity'], scatter=False, color='red')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#4) Which artist has the highest number of tracks in the dataset? Display the count of tracks for each artist using a countplot?\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "# Assuming the dataset is named 'spotify_tracks.csv'\n",
        "df = pd.read_csv('spotify_tracks.csv')\n",
        "\n",
        "# Group by artist and count the number of tracks\n",
        "artist_track_count = df['Artist'].value_counts()\n",
        "\n",
        "# Identify the artist with the highest number of tracks\n",
        "top_artist = artist_track_count.idxmax()\n",
        "top_artist_count = artist_track_count.max()\n",
        "\n",
        "print(f\"Artist with the highest number of tracks: {top_artist} ({top_artist_count} tracks)\")\n",
        "\n",
        "# Plot countplot for the top artists (if too many artists, show the top 10 by track count)\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(data=df, x='Artist', order=artist_track_count.index[:10], palette='viridis')\n",
        "\n",
        "# Enhance plot\n",
        "plt.title('Count of Tracks by Artist')\n",
        "plt.xlabel('Artist')\n",
        "plt.ylabel('Number of Tracks')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#5) What are the top 5 least popular tracks in the dataset? Provide the artist name and track name for each?\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "# Assuming the dataset is named 'spotify_tracks.csv'\n",
        "df = pd.read_csv('spotify_tracks.csv')\n",
        "\n",
        "# Check for null values in the 'Popularity' column\n",
        "if df['Popularity'].isnull().sum() > 0:\n",
        "    print(\"Null values found in 'Popularity'. Dropping rows with null values.\")\n",
        "    df = df.dropna(subset=['Popularity'])\n",
        "\n",
        "# Sort the dataset by 'Popularity' in ascending order\n",
        "least_popular_tracks = df.sort_values(by='Popularity', ascending=True).head(5)\n",
        "\n",
        "# Display artist name and track name for the bottom 5 tracks\n",
        "print(\"Top 5 Least Popular Tracks:\")\n",
        "print(least_popular_tracks[['Artist', 'Track Name', 'Popularity']])\n",
        "\n",
        "\n",
        "#6) Among the top 5 most popular artists, which artist has the highest popularity on average? Calculate and display the average popularity for each artist?\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "# Assuming the dataset is named 'spotify_tracks.csv'\n",
        "df = pd.read_csv('spotify_tracks.csv')\n",
        "\n",
        "# Check for null values in 'Popularity' column\n",
        "if df['Popularity'].isnull().sum() > 0:\n",
        "    print(\"Null values found in 'Popularity'. Dropping rows with null values.\")\n",
        "    df = df.dropna(subset=['Popularity'])\n",
        "\n",
        "# Group by artist and calculate average popularity\n",
        "artist_avg_popularity = df.groupby('Artist')['Popularity'].mean().sort_values(ascending=False)\n",
        "\n",
        "# Identify the top 5 most popular artists\n",
        "top_5_artists = artist_avg_popularity.head(5)\n",
        "\n",
        "# Display the results\n",
        "print(\"Top 5 Most Popular Artists and Their Average Popularity:\")\n",
        "print(top_5_artists)\n",
        "\n",
        "# Identify the artist with the highest average popularity\n",
        "top_artist = top_5_artists.idxmax()\n",
        "top_avg_popularity = top_5_artists.max()\n",
        "print(f\"\\nArtist with the highest average popularity: {top_artist} (Average Popularity: {top_avg_popularity:.2f})\")\n",
        "\n",
        "\n",
        "#7) For the top 5 most popular artists, what are their most popular tracks? List the track name for each artist?\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "# Assuming the dataset is named 'spotify_tracks.csv'\n",
        "df = pd.read_csv('spotify_tracks.csv')\n",
        "\n",
        "# Check for null values in 'Popularity' column\n",
        "if df['Popularity'].isnull().sum() > 0:\n",
        "    print(\"Null values found in 'Popularity'. Dropping rows with null values.\")\n",
        "    df = df.dropna(subset=['Popularity'])\n",
        "\n",
        "# Group by artist and calculate average popularity\n",
        "artist_avg_popularity = df.groupby('Artist')['Popularity'].mean().sort_values(ascending=False)\n",
        "\n",
        "# Identify the top 5 most popular artists\n",
        "top_5_artists = artist_avg_popularity.head(5).index\n",
        "\n",
        "# Find the most popular track for each artist\n",
        "most_popular_tracks = []\n",
        "for artist in top_5_artists:\n",
        "    artist_tracks = df[df['Artist'] == artist]\n",
        "    most_popular_track = artist_tracks.loc[artist_tracks['Popularity'].idxmax()]\n",
        "    most_popular_tracks.append({'Artist': artist, 'Track Name': most_popular_track['Track Name']})\n",
        "\n",
        "# Convert the results to a DataFrame for display\n",
        "result_df = pd.DataFrame(most_popular_tracks)\n",
        "\n",
        "# Display the results\n",
        "print(\"Most Popular Track for Each of the Top 5 Artists:\")\n",
        "print(result_df)\n",
        "\n",
        "\n",
        "#8) Visualize relationships between multiple numerical variables simultaneously using a pair plot?\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset from the uploaded PDF (converted to table-friendly format)\n",
        "data_file_path = \"/mnt/data/EDA 4.pdf\"\n",
        "\n",
        "# Assuming the file contains structured table data, let's extract and clean it\n",
        "try:\n",
        "    # Load the dataset assuming tabular structure (adjust based on actual file content)\n",
        "    data = pd.read_csv(data_file_path, sep=\",\")  # Hypothetical delimiter; may need adjustment\n",
        "except Exception as e:\n",
        "    data = None\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "\n",
        "# If the dataset loaded successfully, inspect and visualize numerical relationships\n",
        "if data is not None:\n",
        "    # Display column names for reference\n",
        "    data_columns = data.columns.tolist()\n",
        "    print(\"Columns in the dataset:\", data_columns)\n",
        "\n",
        "    # Ensure only numerical columns are used for the pair plot\n",
        "    numerical_data = data.select_dtypes(include=['int64', 'float64'])\n",
        "\n",
        "    # Generate the pair plot\n",
        "    if not numerical_data.empty:\n",
        "        sns.pairplot(numerical_data)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No numerical data available for visualization.\")\n",
        "\n",
        "#(9) Does the duration of tracks vary significantly across different artists? Explore this visually using a box plot or violin plot?\n",
        "\n",
        "import fitz  # PyMuPDF\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract text content from the uploaded PDF\n",
        "pdf_path = \"/mnt/data/EDA 4.pdf\"\n",
        "\n",
        "try:\n",
        "    # Read PDF and extract text\n",
        "    pdf_document = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page_num in range(len(pdf_document)):\n",
        "        text += pdf_document[page_num].get_text()\n",
        "    pdf_document.close()\n",
        "\n",
        "    # Display extracted text for verification\n",
        "    print(\"Text extraction successful.\")\n",
        "except Exception as e:\n",
        "    text = None\n",
        "    print(f\"Error extracting text from PDF: {e}\")\n",
        "\n",
        "# Process the extracted text to create a DataFrame (based on observed content structure)\n",
        "if text:\n",
        "    # Attempt to clean and parse into a structured DataFrame\n",
        "    try:\n",
        "        # Manually specify the column headers from observed data\n",
        "        column_headers = [\"Artist\", \"Track Name\", \"Popularity\", \"Duration (ms)\", \"Track ID\"]\n",
        "\n",
        "        # Parse rows into a list (adjust depending on format seen in the text)\n",
        "        rows = [line.split() for line in text.split('\\n') if len(line.split()) > 4]\n",
        "\n",
        "        # Construct the DataFrame\n",
        "        data = pd.DataFrame(rows, columns=column_headers)\n",
        "\n",
        "        # Convert numerical columns to appropriate types\n",
        "        data[\"Popularity\"] = pd.to_numeric(data[\"Popularity\"], errors='coerce')\n",
        "        data[\"Duration (ms)\"] = pd.to_numeric(data[\"Duration (ms)\"], errors='coerce')\n",
        "\n",
        "        # Display the DataFrame head for verification\n",
        "        print(data.head())\n",
        "\n",
        "        # Create a box plot or violin plot for track duration by artist\n",
        "        plt.figure(figsize=(15, 8))\n",
        "        sns.violinplot(data=data, x=\"Artist\", y=\"Duration (ms)\", palette=\"muted\")\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.title(\"Track Duration Across Artists\")\n",
        "        plt.ylabel(\"Duration (ms)\")\n",
        "        plt.xlabel(\"Artist\")\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating DataFrame or visualizing: {e}\")\n",
        "\n",
        "\n",
        "# ( 10) How does the distribution of track popularity vary for different artists? Visualize this using a swarm plot or a violin plot.?\n",
        "\n",
        "#Dataset Format: Ensure the dataset is structured in tabular format with columns like Artist, Popularity, etc.\n",
        "#If the dataset is uploaded as a CSV or Excel file, it can be directly processed.\n",
        "#Generate Visualization:\n",
        "#A violin plot or swarm plot will group popularity values by artists and visually represent the distribution.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}